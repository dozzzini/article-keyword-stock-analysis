{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbee3c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting krwordrank\n",
      "  Downloading krwordrank-1.0.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.18.4 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from krwordrank) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from krwordrank) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from krwordrank) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.1->krwordrank) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.1->krwordrank) (3.5.0)\n",
      "Downloading krwordrank-1.0.3-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: krwordrank\n",
      "Successfully installed krwordrank-1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install krwordrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04dddf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "=== 필터링 후 Top 30 키워드 ===\n",
      "코스피\n",
      "ai\n",
      "반도체\n",
      "삼성전자\n",
      "외국인\n",
      "기관\n",
      "엔비디아\n",
      "hbm\n",
      "삼성\n",
      "증시\n",
      "외인\n",
      "만닉스\n",
      "특징주\n",
      "삼전\n",
      "실적\n",
      "메모리\n",
      "환율\n",
      "만원\n",
      "마감시황\n",
      "코스닥\n",
      "최태원\n",
      "속보\n",
      "개장시황\n",
      "투자\n",
      "훈풍에\n",
      "글로벌\n",
      "시대\n",
      "주가\n",
      "시총\n",
      "거품론\n",
      "출력 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1) CSV 불러오기\n",
    "df = pd.read_csv(\"../mini_project1/preprocessed_data/4분기_일자별_전처리1.csv\")\n",
    "\n",
    "target_col = \"title\"   # 필요하면 \"제목\"으로 바꿔도 됨\n",
    "df[target_col] = df[target_col].fillna(\"\")\n",
    "\n",
    "# 2) 불용어 - 긍정/부정 관련\n",
    "stopwords = {\n",
    "    # 1. 시간/시기/접속사 관련 (Time/Conjunction)\n",
    "    '장중', '만에', '연속', '마감', '출발', \n",
    "    '이날', '오전', '오후', '하루', '주간', '월간', '올해', '내년', '작년', '지난해',\n",
    "    '앞두고', '가운데', '속', '종합',# <--- 추가된 단어들\n",
    "\n",
    "    # 2. 상태/등락/감성 관련 (State/Fluctuation) -> 감성 사전으로 이동할 것들\n",
    "    '상승', '급등', '급락', '반등', '회복', '돌파', '동반', '경신', '확대', '용인',\n",
    "    '강보합', '약보합', '약세', '강세', '보합', '혼조', '하락', '갱신', '지속', '쇼크',\n",
    "    '붕괴', '추락', '급감', '폭락', # <--- 추가된 단어들 ('붕괴' 포함)\n",
    "\n",
    "    # 3. 수량/정도/행동 (Quantity/Action)\n",
    "    '최대', '최고치', '사상', '기대', '매수', '전망', '우려', '부진', '개월', '조원', '기대감', '연고점','최고',\n",
    "\n",
    "    # 4. 동사\n",
    "    '사자', '코스닥도', '전환', '팔자', '매수에',\n",
    "\n",
    "    # 5. 기타\n",
    "    'sk', '분기', '하이닉스', '만전자'\n",
    "\n",
    "}\n",
    "\n",
    "# 3) 토크나이저: 숫자 제거, 한 글자 제거, 불용어 제거\n",
    "def tokenize(text: str):\n",
    "    # 한글/영문만 남기고 나머지 제거\n",
    "    text = re.sub(r\"[^가-힣A-Za-z ]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for w in tokens:\n",
    "        # 한 글자 제거\n",
    "        if len(w) < 2:\n",
    "            continue\n",
    "        # 숫자 포함 단어 제거\n",
    "        if re.search(r\"[0-9]\", w):\n",
    "            continue\n",
    "        # 불용어 제거\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        # 'sk하이닉스'가 포함된 형태들 제거 (예: sk하이닉스가, sk하이닉스는)\n",
    "        if \"sk하이닉스\" in w:\n",
    "            continue\n",
    "\n",
    "        clean_tokens.append(w)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "# 4) TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    token_pattern=None  # 우리가 직접 tokenizer 쓸 거라 None\n",
    ")\n",
    "\n",
    "docs = df[target_col].astype(str).tolist() # 기사들 추출\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "n_docs = tfidf_matrix.shape[0]\n",
    "\n",
    "# 5) 문서비율(df) 계산 (해당 단어가 등장한 문서 비율)\n",
    "doc_freq = (tfidf_matrix > 0).sum(axis=0).A.ravel()\n",
    "doc_ratio = doc_freq / n_docs\n",
    "\n",
    "# df 필터: 1% 이상, 50% 이하만 사용 (너무 흔하거나 너무 희귀한 단어 제거)\n",
    "min_df_ratio = 0.01   # 1%\n",
    "max_df_ratio = 0.50   # 50%\n",
    "\n",
    "valid_mask = (doc_ratio >= min_df_ratio) & (doc_ratio <= max_df_ratio)\n",
    "valid_indices = np.where(valid_mask)[0]\n",
    "\n",
    "valid_features = feature_names[valid_indices] # 1차적으로 뽑힌 단어들\n",
    "# print('필터링 되기 전:',valid_features)\n",
    "valid_tfidf = tfidf_matrix[:, valid_indices]\n",
    "\n",
    "# 6) 각 단어의 평균 TF-IDF 점수\n",
    "mean_tfidf = valid_tfidf.mean(axis=0).A.ravel()\n",
    "word_scores = list(zip(valid_features, mean_tfidf)) # 단어 - 가중치 점수 매핑된 거 \n",
    "print(len(word_scores))\n",
    "# print('단어점수:',word_scores)\n",
    "\n",
    "# 7) 평균 TF-IDF 기준 Top 50 단어\n",
    "top30 = sorted(word_scores, key=lambda x: x[1], reverse=True)[:30] # 매핑된 최종 단어들 (여기에서 첫 값들만 추출)\n",
    "# print(top30)\n",
    "\n",
    "print(\"=== 필터링 후 Top 30 키워드 ===\")\n",
    "for w, _ in top30:# 첫 단어들만 추출 \n",
    "    print(w)\n",
    "\n",
    "# Top50 단어 목록만 따로 추출\n",
    "top_words = [w for w, _ in top30] # 나와야하는 단어들 -> list\n",
    "# print(type(top_words))\n",
    "\n",
    "# valid_features 에서 top30 단어의 인덱스 찾기\n",
    "word_to_idx = {w: i for i, w in enumerate(valid_features)}\n",
    "top_indices = [word_to_idx[w] for w in top_words]\n",
    "\n",
    "# 각 문서별 top30 단어 TF-IDF 매트릭스 (n_docs x 50)\n",
    "top_tfidf = valid_tfidf[:, top_indices].toarray()\n",
    "\n",
    "# 8) 문서별 최대값을 1(=100%)로 보이도록 정규화\n",
    "#   - 각 제목에서 가장 높은 TF-IDF = 1.0 → 100%\n",
    "row_max = top_tfidf.max(axis=1, keepdims=True)\n",
    "row_max[row_max == 0] = 1  # 전부 0인 경우 0으로 나누기 방지\n",
    "\n",
    "top_tfidf_pct = (top_tfidf / row_max)  # 이 줄은 여기까지만 쓰고 아래는 새로 추가\n",
    "\n",
    "# 9) TOP50 키워드별로 \"제목에서 등장한 빈도 비율(0~1)\" 계산\n",
    "#    doc_ratio: 전체 vocabulary 기준 (단어가 등장한 문서 수 / 전체 문서 수)\n",
    "#    valid_indices: 그중 df 1%~50% 필터 통과한 단어 인덱스\n",
    "doc_ratio_valid = doc_ratio[valid_indices]  # valid_features와 같은 순서\n",
    "\n",
    "# top_words 각각에 대해 빈도 비율 추출\n",
    "top_freq_ratio = np.array([doc_ratio_valid[word_to_idx[w]] for w in top_words])\n",
    "\n",
    "# 10) CSV 저장: 컬럼 2개 (keyword, ratio)\n",
    "result_df = pd.DataFrame({\n",
    "    \"keyword\": top_words,        # TOP50 키워드\n",
    "    \"ratio\": top_freq_ratio      # 각 키워드가 등장한 제목 비율 (0~1, 1이 100%)\n",
    "})\n",
    "\n",
    "result_df.to_csv(\"../mini_project1/preprocessed_data/4분기_키워드_top30.csv\",\n",
    "                 index=False,\n",
    "                 encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"출력 완료\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
